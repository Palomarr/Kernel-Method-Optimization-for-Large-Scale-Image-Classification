{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Kernel Methods & Random Fourier Features\n",
    "\n",
    "In this notebook we compare the performance of an exact kernel classifier with a kernel approximation using Random Fourier Features (RFF) on the Sign Language MNIST dataset. We will assess the methods in terms of accuracy, training time, prediction time, and memory usage. Later sections explore scaling behavior and extrapolate the computational requirements for applying these methods to the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Ignore warnings for clarity\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root directory to the path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import our modules\n",
    "from kernel_methods.utils.data_loader import get_sample_dataset, load_processed_data\n",
    "from kernel_methods.models.kernel_base import KernelClassifier\n",
    "from kernel_methods.models.kernel_approximation_classifier import KernelApproximationClassifier\n",
    "\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print('Imports successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Data\n",
    "\n",
    "We begin by loading a small sample of the Sign Language MNIST dataset for quick experimentation. We will print the shapes of the training and validation sets along with the class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample of the data\n",
    "X_train_sample, X_val_sample, y_train_sample, y_val_sample = get_sample_dataset(n_samples=500, random_state=42)\n",
    "\n",
    "print(f\"Training sample shape: {X_train_sample.shape}\")\n",
    "print(f\"Validation sample shape: {X_val_sample.shape}\")\n",
    "\n",
    "print(\"\\nClass distribution in training sample:\")\n",
    "unique, counts = np.unique(y_train_sample, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"Label {label}: {count} samples ({count/len(y_train_sample)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline: Exact Kernel Method\n",
    "\n",
    "We now train an exact kernel classifier using an RBF kernel. This classifier computes the full kernel matrix which becomes expensive for large datasets. The performance of this exact method will be our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the exact kernel classifier\n",
    "kernel_clf = KernelClassifier(kernel='rbf', gamma=0.01, C=1.0)\n",
    "\n",
    "# Train the classifier\n",
    "start_time = time.time()\n",
    "kernel_clf.fit(X_train_sample, y_train_sample)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "# Predict on the validation set\n",
    "start_time = time.time()\n",
    "y_val_pred = kernel_clf.predict(X_val_sample)\n",
    "predict_time = time.time() - start_time\n",
    "\n",
    "# Calculate accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_val_sample, y_val_pred)\n",
    "\n",
    "# Log results\n",
    "exact_metrics = kernel_clf.get_performance_metrics()\n",
    "print(f\"Exact Kernel Method Results:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Training time: {train_time:.4f} seconds\")\n",
    "print(f\"Prediction time: {predict_time:.4f} seconds\")\n",
    "print(f\"Estimated memory usage: {exact_metrics['memory_usage_mb']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Confusion Matrix\n",
    "\n",
    "A confusion matrix helps us better understand the classification performance. We use a heatmap to visualize the normalized confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names=None, figsize=(14, 12), title='Confusion Matrix'):\n",
    "    \"\"\"\n",
    "    Plot a confusion matrix with clear boundary lines between cells.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        Ground truth (correct) target values.\n",
    "    y_pred : array-like\n",
    "        Estimated targets as returned by a classifier.\n",
    "    class_names : list, optional\n",
    "        List of class names for axis labels.\n",
    "    figsize : tuple, optional\n",
    "        Figure size (width, height) in inches.\n",
    "    title : str, optional\n",
    "        Title of the confusion matrix plot.\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Normalize the confusion matrix\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Create figure and axes\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Generate class names if not provided\n",
    "    if class_names is None:\n",
    "        class_names = [str(i) for i in range(len(np.unique(np.concatenate([y_true, y_pred]))))]\n",
    "    \n",
    "    # Create prettier plot with seaborn - important changes for boundary lines:\n",
    "    # 1. Increased linewidths parameter (from 0.5 to 1.0)\n",
    "    # 2. Added linecolor parameter to make boundaries more visible\n",
    "    ax = sns.heatmap(\n",
    "        cm_normalized, \n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "        cmap='Blues',\n",
    "        square=True,\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        linewidths=1.0,         # Thicker lines between cells\n",
    "        linecolor='black',      # Black boundary lines for contrast\n",
    "        cbar_kws={\"shrink\": .8}\n",
    "    )\n",
    "    \n",
    "    # Add a strong outer border to the entire matrix\n",
    "    for _, spine in ax.spines.items():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_linewidth(2)\n",
    "        spine.set_color('black')\n",
    "    \n",
    "    # Improve aesthetics\n",
    "    plt.title(title, fontsize=16, pad=20)\n",
    "    plt.ylabel('True Label', fontsize=14, labelpad=10)\n",
    "    plt.xlabel('Predicted Label', fontsize=14, labelpad=10)\n",
    "    \n",
    "    # Rotate the tick labels and set alignment\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\", fontweight='bold')\n",
    "    plt.setp(ax.get_yticklabels(), fontweight='bold')\n",
    "    \n",
    "    # Tight layout to ensure everything fits\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()\n",
    "    \n",
    "    # Also return the raw confusion matrix for further analysis if needed\n",
    "    return cm\n",
    "\n",
    "alphabet = [chr(i+65) for i in range(26) if chr(i+65) not in ['J', 'Z']]\n",
    "\n",
    "# Plot the improved confusion matrix\n",
    "plot_confusion_matrix(\n",
    "    y_val_sample, \n",
    "    y_val_pred,\n",
    "    class_names=alphabet,\n",
    "    title='Sign Language Recognition - Exact Kernel Method'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strengths of the Model\n",
    "- Perfect Classification for several letters:\n",
    "    - B, N, P, and V (1.00 accuracy)\n",
    "    - Letters with distinctive hand shapes show the strongest performance\n",
    "- Strong Performance on:\n",
    "    - Letter T (0.88)\n",
    "    - Letter Q (0.83)\n",
    "    - Letters C, H, R, Y (0.67)\n",
    "    - Letter L (0.75)\n",
    "\n",
    "#### Key Challenges\n",
    "- Complete Misclassification of letter A (0.0 on diagonal)\n",
    "- Poor Recognition of:\n",
    "    - Letter E (scattered across multiple classes)\n",
    "    - Letter U (distributed across several predictions)\n",
    "    - Letter W (fragmented across many classes)\n",
    "    - Letter X (appears missing from diagonal)\n",
    "\n",
    "The confusion patterns strongly correlate with visual similarities in ASL hand shapes:\n",
    "- Letters with similar finger positions or orientations are frequently confused\n",
    "- Letters with unique, distinctive shapes (B, N, P, V) achieve perfect recognition\n",
    "- Letters requiring subtle distinctions show higher confusion rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Fourier Features Approximation\n",
    "\n",
    "Next, we use Random Fourier Features (RFF) to approximate the RBF kernel. This transformation maps the data into a lower-dimensional space where an inner product approximates the RBF kernel. We test several component sizes to explore the accuracy–performance trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of RFF component sizes to try\n",
    "component_sizes = [50, 100, 200, 500]\n",
    "\n",
    "# Store RFF results\n",
    "rff_results = []\n",
    "\n",
    "for n_components in component_sizes:\n",
    "    print(f\"\\nTraining RFF with {n_components} components...\")\n",
    "    \n",
    "    # Initialize RFF classifier with kernel approximation\n",
    "    rff_clf = KernelApproximationClassifier(\n",
    "        approximation='rff',\n",
    "        n_components=n_components,\n",
    "        gamma=0.01,\n",
    "        C=1.0,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train classifier\n",
    "    start_time = time.time()\n",
    "    rff_clf.fit(X_train_sample, y_train_sample)\n",
    "    train_time_rff = time.time() - start_time\n",
    "    \n",
    "    # Evaluate classifier\n",
    "    start_time = time.time()\n",
    "    y_val_pred_rff = rff_clf.predict(X_val_sample)\n",
    "    predict_time_rff = time.time() - start_time\n",
    "    \n",
    "    # Compute accuracy\n",
    "    acc_rff = accuracy_score(y_val_sample, y_val_pred_rff)\n",
    "    \n",
    "    # Estimate memory usage for RFF (approximation formula)\n",
    "    memory_usage_rff = (X_train_sample.shape[1] * n_components * 8) / (1024 * 1024)  # in MB\n",
    "    \n",
    "    print(f\"Accuracy: {acc_rff:.4f}\")\n",
    "    print(f\"Training time: {train_time_rff:.4f} seconds\")\n",
    "    print(f\"Prediction time: {predict_time_rff:.4f} seconds\")\n",
    "    print(f\"Estimated memory usage: {memory_usage_rff:.2f} MB\")\n",
    "    \n",
    "    rff_results.append({\n",
    "        'n_components': n_components,\n",
    "        'accuracy': acc_rff,\n",
    "        'train_time': train_time_rff,\n",
    "        'predict_time': predict_time_rff,\n",
    "        'memory_usage': memory_usage_rff,\n",
    "        'predictions': y_val_pred_rff\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Confusion Matrix for the Best RFF Model\n",
    "\n",
    "We now determine which RFF configuration achieved the highest accuracy and plot its confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best RFF model based on validation accuracy\n",
    "best_rff_idx = np.argmax([result['accuracy'] for result in rff_results])\n",
    "best_rff_model = rff_results[best_rff_idx]\n",
    "best_components = best_rff_model['n_components']\n",
    "best_predictions = best_rff_model['predictions']\n",
    "\n",
    "# Create the alphabet list (A-X, excluding J and Z)\n",
    "alphabet = [chr(i+65) for i in range(26) if chr(i+65) not in ['J', 'Z']]\n",
    "\n",
    "# Plot the confusion matrix for RFF method\n",
    "plot_confusion_matrix(\n",
    "    y_val_sample, \n",
    "    best_predictions,\n",
    "    class_names=alphabet,\n",
    "    title=f'Sign Language Recognition - RFF Method ({best_components} components)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights on the Confusion matrix\n",
    "1. Strong performers: Letters B, K, N, and V have perfect (1.00) classification accuracy, indicating the RFF method works extremely well for these signs.\n",
    "2. Moderate performers: Letters like L (0.75), Y (0.67), C (0.67), and T (0.62) have good but not perfect accuracy.\n",
    "3. Challenging signs: Many letters show poor classification, particularly E, I, G, H, M, and W, which are frequently confused with other letters.\n",
    "4. Specific confusion patterns:\n",
    "    - Letter A is frequently confused with C and P\n",
    "    - Letter D is confused with K (0.50)\n",
    "    - Letter W is often misclassified as K (0.36)\n",
    "    - Letter G is split between being classified as G, F, and S (0.33 each)\n",
    "5. Systematic issues: Some letters appear to have similar hand shapes that the RFF method struggles to differentiate, suggesting that either more components or a different kernel approach might be needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Comparison\n",
    "\n",
    "We now compile the results of the exact kernel method and the various RFF approximations into a comparison table and visualize the differences in accuracy, training time, prediction time, and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build comparison DataFrame\n",
    "comparison_data = {\n",
    "    'Method': ['Exact Kernel'] + [f'RFF-{res[\"n_components\"]}' for res in rff_results],\n",
    "    'Accuracy': [accuracy] + [res['accuracy'] for res in rff_results],\n",
    "    'Training Time (s)': [train_time] + [res['train_time'] for res in rff_results],\n",
    "    'Prediction Time (s)': [predict_time] + [res['predict_time'] for res in rff_results],\n",
    "    'Memory Usage (MB)': [exact_metrics['memory_usage_mb']] + [res['memory_usage'] for res in rff_results]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df)\n",
    "\n",
    "# Plot performance comparisons\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Accuracy\n",
    "sns.barplot(x='Method', y='Accuracy', data=comparison_df, ax=axes[0])\n",
    "axes[0].set_title('Accuracy Comparison')\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45)\n",
    "\n",
    "# Training time\n",
    "sns.barplot(x='Method', y='Training Time (s)', data=comparison_df, ax=axes[1])\n",
    "axes[1].set_title('Training Time Comparison')\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45)\n",
    "\n",
    "# Prediction time\n",
    "sns.barplot(x='Method', y='Prediction Time (s)', data=comparison_df, ax=axes[2])\n",
    "axes[2].set_title('Prediction Time Comparison')\n",
    "axes[2].set_xticklabels(axes[2].get_xticklabels(), rotation=45)\n",
    "\n",
    "# Memory usage\n",
    "sns.barplot(x='Method', y='Memory Usage (MB)', data=comparison_df, ax=axes[3])\n",
    "axes[3].set_title('Memory Usage Comparison')\n",
    "axes[3].set_xticklabels(axes[3].get_xticklabels(), rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "\n",
    "1. Accuracy trade-offs:\n",
    "   - The exact kernel method significantly outperforms all RFF approximations (~50% vs 20-33%)\n",
    "   - Among RFF variants, RFF-200 performs best accuracy-wise, with diminishing returns at RFF-500\n",
    "   - There's a substantial accuracy cost when using the approximation methods\n",
    "\n",
    "2. Computational efficiency:\n",
    "   - Training time: RFF-100 is fastest, while exact kernel and RFF-500 are similarly slow\n",
    "   - Prediction time: All RFF variants are faster than the exact kernel, with RFF-100 being fastest\n",
    "   - Memory usage: RFF methods use significantly less memory (RFF-50 uses only ~6% of exact kernel's memory)\n",
    "\n",
    "3. Scaling characteristics:\n",
    "   - Adding more components generally increases accuracy up to a point (200 components)\n",
    "   - Higher component counts directly increase memory usage and training time\n",
    "   - The efficiency benefits of RFF diminish at 500 components\n",
    "\n",
    "4. Optimal configurations:\n",
    "   - RFF-100 offers the best prediction speed\n",
    "   - RFF-200 provides the best balance between accuracy and resource usage\n",
    "   - The exact kernel should be used when accuracy is the only priority\n",
    "\n",
    "This demonstrates the fundamental trade-off in kernel methods: exact computation provides better accuracy but at higher computational cost, while approximation methods sacrifice some accuracy for significant gains in speed and memory efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scaling Analysis\n",
    "\n",
    "To investigate scalability, we run experiments on subsets of increasing size. This analysis reveals how the training time, prediction time, memory usage, and accuracy change as the number of samples increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Define sample sizes for scaling analysis\n",
    "sample_sizes = [100, 200, 500, 1000, 2000]\n",
    "scaling_results = []\n",
    "\n",
    "# Use best RFF component setting from previous experiments\n",
    "best_n_components = rff_results[best_rff_idx]['n_components']\n",
    "\n",
    "for size in tqdm(sample_sizes, desc=\"Testing different sample sizes\"):\n",
    "    print(f\"\\nTesting with sample size: {size}\")\n",
    "    \n",
    "    # Get a subset of data\n",
    "    X_train_subset, X_val_subset, y_train_subset, y_val_subset = get_sample_dataset(n_samples=size, random_state=42)\n",
    "    result = {'sample_size': size}\n",
    "    \n",
    "    # Test exact kernel method\n",
    "    try:\n",
    "        exact_clf = KernelClassifier(kernel='rbf', gamma=0.01, C=1.0)\n",
    "        start_time = time.time()\n",
    "        exact_clf.fit(X_train_subset, y_train_subset)\n",
    "        exact_train_time = time.time() - start_time\n",
    "        \n",
    "        start_time = time.time()\n",
    "        y_pred_exact = exact_clf.predict(X_val_subset)\n",
    "        exact_predict_time = time.time() - start_time\n",
    "        \n",
    "        acc_exact = accuracy_score(y_val_subset, y_pred_exact)\n",
    "        exact_metrics = exact_clf.get_performance_metrics()\n",
    "        \n",
    "        result.update({\n",
    "            'exact_train_time': exact_train_time,\n",
    "            'exact_predict_time': exact_predict_time,\n",
    "            'exact_accuracy': acc_exact,\n",
    "            'exact_memory': exact_metrics['memory_usage_mb']\n",
    "        })\n",
    "        \n",
    "        print(f\"Exact Kernel - Accuracy: {acc_exact:.4f}, Training time: {exact_train_time:.4f}s\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with exact kernel for size {size}: {e}\")\n",
    "        result.update({\n",
    "            'exact_train_time': float('nan'),\n",
    "            'exact_predict_time': float('nan'),\n",
    "            'exact_accuracy': float('nan'),\n",
    "            'exact_memory': float('nan')\n",
    "        })\n",
    "    \n",
    "    # Test RFF approximation\n",
    "    rff_clf = KernelApproximationClassifier(\n",
    "        approximation='rff',\n",
    "        n_components=best_n_components,\n",
    "        gamma=0.01,\n",
    "        C=1.0,\n",
    "        random_state=42\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    rff_clf.fit(X_train_subset, y_train_subset)\n",
    "    rff_train_time = time.time() - start_time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    y_pred_rff = rff_clf.predict(X_val_subset)\n",
    "    rff_predict_time = time.time() - start_time\n",
    "    \n",
    "    acc_rff = accuracy_score(y_val_subset, y_pred_rff)\n",
    "    rff_memory = (X_train_subset.shape[1] * best_n_components * 8) / (1024 * 1024)\n",
    "    \n",
    "    result.update({\n",
    "        'rff_train_time': rff_train_time,\n",
    "        'rff_predict_time': rff_predict_time,\n",
    "        'rff_accuracy': acc_rff,\n",
    "        'rff_memory': rff_memory\n",
    "    })\n",
    "    \n",
    "    print(f\"RFF ({best_n_components} components) - Accuracy: {acc_rff:.4f}, Training time: {rff_train_time:.4f}s\")\n",
    "    \n",
    "    scaling_results.append(result)\n",
    "\n",
    "scaling_df = pd.DataFrame(scaling_results)\n",
    "print(scaling_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Training time\n",
    "axes[0].set_title('Training Time vs Dataset Size')\n",
    "axes[0].set_xlabel('Number of Training Samples')\n",
    "axes[0].set_ylabel('Training Time (s)')\n",
    "valid_exact = ~np.isnan(scaling_df['exact_train_time'])\n",
    "axes[0].plot(scaling_df['sample_size'][valid_exact], scaling_df['exact_train_time'][valid_exact], 'o-', label='Exact Kernel')\n",
    "axes[0].plot(scaling_df['sample_size'], scaling_df['rff_train_time'], 'o-', label=f'RFF ({best_n_components} components)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Prediction time\n",
    "axes[1].set_title('Prediction Time vs Dataset Size')\n",
    "axes[1].set_xlabel('Number of Training Samples')\n",
    "axes[1].set_ylabel('Prediction Time (s)')\n",
    "valid_exact_pred = ~np.isnan(scaling_df['exact_predict_time'])\n",
    "axes[1].plot(scaling_df['sample_size'][valid_exact_pred], scaling_df['exact_predict_time'][valid_exact_pred], 'o-', label='Exact Kernel')\n",
    "axes[1].plot(scaling_df['sample_size'], scaling_df['rff_predict_time'], 'o-', label=f'RFF ({best_n_components} components)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Memory usage\n",
    "axes[2].set_title('Memory Usage vs Dataset Size')\n",
    "axes[2].set_xlabel('Number of Training Samples')\n",
    "axes[2].set_ylabel('Memory Usage (MB)')\n",
    "valid_exact_mem = ~np.isnan(scaling_df['exact_memory'])\n",
    "axes[2].plot(scaling_df['sample_size'][valid_exact_mem], scaling_df['exact_memory'][valid_exact_mem], 'o-', label='Exact Kernel')\n",
    "axes[2].plot(scaling_df['sample_size'], scaling_df['rff_memory'], 'o-', label=f'RFF ({best_n_components} components)')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True)\n",
    "\n",
    "# Accuracy\n",
    "axes[3].set_title('Accuracy vs Dataset Size')\n",
    "axes[3].set_xlabel('Number of Training Samples')\n",
    "axes[3].set_ylabel('Accuracy')\n",
    "valid_exact_acc = ~np.isnan(scaling_df['exact_accuracy'])\n",
    "axes[3].plot(scaling_df['sample_size'][valid_exact_acc], scaling_df['exact_accuracy'][valid_exact_acc], 'o-', label='Exact Kernel')\n",
    "axes[3].plot(scaling_df['sample_size'], scaling_df['rff_accuracy'], 'o-', label=f'RFF ({best_n_components} components)')\n",
    "axes[3].legend()\n",
    "axes[3].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "1. Computational scaling:\n",
    "   - Exact kernel exhibits quadratic scaling (O(n²)) for training time, increasing dramatically from 0.5s to 3.1s as samples grow\n",
    "   - RFF training time remains nearly constant (around 0.1s) regardless of dataset size\n",
    "   - At 2000 samples, exact kernel is ~30× slower than RFF for training\n",
    "\n",
    "2. Memory efficiency:\n",
    "   - Exact kernel memory usage grows quadratically, reaching ~43MB at 2000 samples\n",
    "   - RFF maintains effectively constant memory usage (~1MB) across all dataset sizes\n",
    "   - The memory gap becomes extreme at scale (>40× difference at 2000 samples)\n",
    "\n",
    "3. Prediction performance:\n",
    "   - Exact kernel prediction time scales linearly with dataset size\n",
    "   - RFF prediction time increases very slowly, maintaining a ~5× speed advantage at 2000 samples\n",
    "   - This prediction efficiency becomes critical for real-time applications\n",
    "\n",
    "4. Accuracy-efficiency tradeoff:\n",
    "   - Both methods show improved accuracy with more training data\n",
    "   - The exact kernel maintains a consistent accuracy advantage (~20%)\n",
    "   - The accuracy gap doesn't widen with scale, suggesting RFF's performance disadvantage is stable\n",
    "\n",
    "This analysis demonstrates why RFF is valuable for large-scale applications: while sacrificing some accuracy, it provides massive scalability benefits that make kernel methods practical for large datasets where exact kernel calculations would be prohibitively expensive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Extrapolation to Full Dataset\n",
    "\n",
    "Finally, we estimate the computational requirements for training on the full Sign Language MNIST dataset. We use a quadratic fit for the exact kernel method and a linear fit for the RFF approximation to predict training time and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full dataset to get the total number of samples (without loading entire data into memory for this demo)\n",
    "X_train_full, _, _, _, _, _ = load_processed_data()\n",
    "full_dataset_size = len(X_train_full)\n",
    "print(f\"Full training dataset size: {full_dataset_size} samples\")\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def estimate_for_full_dataset(df):\n",
    "    # Estimate for exact kernel method using a quadratic fit\n",
    "    valid_exact = ~np.isnan(df['exact_train_time'])\n",
    "    if sum(valid_exact) >= 3:\n",
    "        X_poly = PolynomialFeatures(degree=2).fit_transform(df['sample_size'][valid_exact].values.reshape(-1, 1))\n",
    "        model_quad = LinearRegression().fit(X_poly, df['exact_train_time'][valid_exact])\n",
    "        X_full_poly = PolynomialFeatures(degree=2).fit_transform(np.array([[full_dataset_size]]))\n",
    "        exact_train_time_est = model_quad.predict(X_full_poly)[0]\n",
    "        \n",
    "        model_mem = LinearRegression().fit(X_poly, df['exact_memory'][valid_exact])\n",
    "        exact_memory_est = model_mem.predict(X_full_poly)[0]\n",
    "        print(f\"Estimated resources for exact kernel method on full dataset ({full_dataset_size} samples):\")\n",
    "        print(f\"  Training time: {exact_train_time_est:.2f} seconds ({exact_train_time_est/60:.2f} minutes)\")\n",
    "        print(f\"  Memory usage: {exact_memory_est:.2f} MB ({exact_memory_est/1024:.2f} GB)\")\n",
    "    else:\n",
    "        exact_train_time_est = float('nan')\n",
    "        exact_memory_est = float('nan')\n",
    "        print(\"Not enough data points to estimate exact kernel method resources.\")\n",
    "        \n",
    "    # Estimate for RFF (linear scaling assumed)\n",
    "    model_rff = LinearRegression().fit(df['sample_size'].values.reshape(-1, 1), df['rff_train_time'])\n",
    "    rff_train_time_est = model_rff.predict([[full_dataset_size]])[0]\n",
    "    \n",
    "    model_rff_mem = LinearRegression().fit(df['sample_size'].values.reshape(-1, 1), df['rff_memory'])\n",
    "    rff_memory_est = model_rff_mem.predict([[full_dataset_size]])[0]\n",
    "    \n",
    "    print(f\"\\nEstimated resources for RFF ({best_n_components} components) on full dataset ({full_dataset_size} samples):\")\n",
    "    print(f\"  Training time: {rff_train_time_est:.2f} seconds ({rff_train_time_est/60:.2f} minutes)\")\n",
    "    print(f\"  Memory usage: {rff_memory_est:.2f} MB ({rff_memory_est/1024:.2f} GB)\")\n",
    "    \n",
    "    return {\n",
    "        'exact_train_time_est': exact_train_time_est,\n",
    "        'exact_memory_est': exact_memory_est,\n",
    "        'rff_train_time_est': rff_train_time_est,\n",
    "        'rff_memory_est': rff_memory_est\n",
    "    }\n",
    "\n",
    "estimates = estimate_for_full_dataset(scaling_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This extrapolation analysis demonstrates the dramatic real-world impact of algorithmic scaling properties:\n",
    "\n",
    "1. Training efficiency contrast:\n",
    "   - Exact kernel: 8.86 minutes (531.66 seconds)\n",
    "   - RFF (200): 0.02 minutes (1.04 seconds)\n",
    "   - **512× speed improvement** with approximation\n",
    "\n",
    "2. Memory requirements:\n",
    "   - Exact kernel: 3.73 GB (3816.12 MB)\n",
    "   - RFF (200): 0.001 GB (1.20 MB)\n",
    "   - **3,180× memory reduction** with approximation\n",
    "\n",
    "3. Practical implications:\n",
    "   - The exact kernel method is on the edge of feasibility for standard hardware\n",
    "   - RFF transforms a substantial computational problem into a trivial one\n",
    "   - The exact method would struggle with even slightly larger datasets\n",
    "\n",
    "4. Scaling relationships:\n",
    "   - The 11× increase in data size (2,000 → 21,964 samples) caused:\n",
    "     - ~171× increase in exact kernel training time \n",
    "     - ~97× increase in exact kernel memory usage\n",
    "   - Confirming worse-than-linear (quadratic) scaling behavior\n",
    "\n",
    "This analysis makes clear why kernel approximation methods are essential for practical machine learning applications, enabling the use of kernel-based approaches on datasets that would otherwise be completely impractical to process using exact methods.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
