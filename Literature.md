## Random Fourier Features

- [Rahimi, A., & Recht, B. (2007). "Random features for large-scale kernel machines." Advances in Neural Information Processing Systems (NIPS)](https://people.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf).

- This is the seminal paper introducing Random Fourier Features, which is a core component of your project.

## Nyström Method

- [Williams, C. K., & Seeger, M. (2001). "Using the Nyström method to speed up kernel machines." Advances in Neural Information Processing Systems.](https://papers.nips.cc/paper_files/paper/2000/file/19de10adbaa1b2ee13f77f679fa1483a-Paper.pdf) 

- Introduces the Nyström approximation for kernel methods.

## Stochastic Approximations

- [Dai, B., Xie, B., He, N., Liang, Y., Raj, A., Balcan, M. F., & Song, L. (2014). "Scalable kernel methods via doubly stochastic gradients." Advances in Neural Information Processing Systems.](https://proceedings.neurips.cc/paper_files/paper/2014/file/c6cc81e8589ebb6accf27b78afad82d9-Paper.pdf) 

- Introduces stochastic optimization approaches for kernel methods.

## Comparison with Deep Learning

- [Huang, P. S., Avron, H., Sainath, T. N., Sindhwani, V., & Ramabhadran, B. (2014). "Kernel methods match deep neural networks on TIMIT." 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).](https://ieeexplore.ieee.org/abstract/document/6853587)

- Shows how optimized kernel methods can compete with neural networks.

## Optimization Techniques

- [Lu, Z., May, A., Liu, K., Garakani, A. B., Guo, D., Bellet, A., Fan, L., Collins, M., Kingsbury, B., Picheny, M., & Sha, F. (2014). "How to scale up kernel methods to be as good as deep neural nets." arXiv preprint arXiv:1411.4000.](https://arxiv.org/pdf/1411.4000)

- Discusses scaling strategies for kernel methods to compete with deep learning.
